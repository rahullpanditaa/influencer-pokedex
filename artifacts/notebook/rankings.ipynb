{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94de2897",
   "metadata": {},
   "source": [
    "# Ranking prototype (BM25 + Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from rank_bm25 import BM25Okapi\n",
    "from pathlib import Path\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "DATA_DIR_PATH = Path.cwd().resolve() / \"data\"\n",
    "STOPWORDS_FILE_PATH = DATA_DIR_PATH / \"stopwords.txt\"\n",
    "CHROMA_DIR_PATH = Path.cwd().resolve() / \"vector_store\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset of 10 creators\n",
    "# dict where key = creator niche, value = creator bio\n",
    "\n",
    "sample_dataset = {\n",
    "    \"fashion_minimalist_style\": \"\"\"I post daily outfit inspirations, seasonal lookbooks, and minimalist fashion tips focused on affordable pieces. \n",
    "My recent content includes capsule wardrobe guides, styling basics, and color-coordination breakdowns. \n",
    "Instagram posts feature outfit photos, thrifted finds, and short styling reels.\n",
    "\"\"\",\n",
    "\n",
    "\"learning_to_code_journey\": \"\"\"I document my journey learning to code from scratch, posting study routines, project logs, and beginner-friendly explanations. \n",
    "Recent videos show progress building Python scripts, simple web apps, and solving DSA problems as a newcomer to tech. \n",
    "Instagram posts include accountability updates, motivational captions, and resources for absolute beginners.\n",
    "\"\"\",\n",
    "\n",
    "\"backend_go_engineer\": \"\"\"I share backend engineering tutorials in Go, focusing on concurrency, networking, microservice design, and Docker-based workflows. \n",
    "My recent content includes deep dives into goroutines, REST API design, production logging, and containerizing real-world services. \n",
    "Instagram posts feature short Go tips, dev memes, and workflow optimizations for backend developers.\n",
    "\"\"\",\n",
    "\n",
    "\"fitness_home_workouts\": \"\"\"I create daily home workout routines targeting fat loss, mobility, and functional strength using bodyweight or minimal equipment. \n",
    "My latest videos include 10-minute HIIT sessions, beginner full-body circuits, and nutrition tips for sustainable weight loss. \n",
    "Instagram posts share motivational progress photos, short workout reels, and simple healthy meal ideas.\n",
    "\"\"\",\n",
    "\n",
    "\"beauty_skincare_reviewer\": \"\"\"I review skincare products and routines for acne-prone, oily, and sensitive skin types, focusing on ingredient science. \n",
    "Recent videos compare retinol serums, sunscreen textures, exfoliants, and Korean skincare routines. \n",
    "Instagram posts share product flatlays, morning/night routines, and short ingredient breakdowns.\n",
    "\"\"\",\n",
    "\n",
    "\"personal_finance_educator\": \"\"\"I simplify personal finance topics such as index investing, budgeting systems, emergency funds, and tax optimization. \n",
    "My recent content includes ETF comparisons, beginner investment strategies, and monthly market breakdowns. \n",
    "Instagram posts include budgeting templates, money habits, and short explainers on compounding and inflation.\n",
    "\"\"\",\n",
    "\n",
    "\"healthy_cooking_mealprep\": \"\"\"I share easy, healthy recipes and weekly meal prep ideas designed for busy students and working professionals. \n",
    "My latest videos feature quick high-protein dinners, 15-minute lunches, and budget-friendly vegetarian meal preps. \n",
    "Instagram posts include grocery hauls, step-by-step meal reels, and simple nutrition tips.\n",
    "\"\"\",\n",
    "\n",
    "\"yoga_mindfulness_coach\": \"\"\"I guide yoga flows, stretching routines, and mindfulness practices aimed at improving mobility, posture, and mental clarity. \n",
    "Recent videos include morning yoga sessions, hip-opening flows, and breathwork techniques for stress relief. \n",
    "Instagram posts share short mobility drills, inspirational quotes, and meditation reminders.\n",
    "\"\"\",\n",
    "\n",
    "\"cloud_devops_engineer\": \"\"\"I break down cloud engineering concepts including AWS, Kubernetes, Docker, and CI/CD automation. \n",
    "My latest tutorials explain Terraform modules, EKS deployments, load balancing, and building secure production pipelines. \n",
    "Instagram posts include cloud diagrams, quick DevOps tips, and workflow comparisons for SREs and platform engineers.\n",
    "\"\"\",\n",
    "\n",
    "\"frontend_react_engineer\": \"\"\"I teach React, TypeScript, and modern frontend engineering with a focus on clean UI patterns and reusable components. \n",
    "Recent videos cover state management, hooks, responsive layouts, and building production-ready interfaces with React and Tailwind. \n",
    "On Instagram, I post quick JavaScript tips, UI design breakdowns, and small project showcases.\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b455c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for pre-processing text for OkapiBM25\n",
    "\n",
    "def remove_all_punctuation_lowercase(text: str) -> str:\n",
    "    tt = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(tt).lower()\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return text.lower().split()\n",
    "\n",
    "def remove_stop_words(tokens: list[str]) -> list[str]:\n",
    "    with open(STOPWORDS_FILE_PATH, \"r\") as f:\n",
    "        stop_words = f.readlines()\n",
    "    \n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        result.append(token)\n",
    "\n",
    "    return result\n",
    "\n",
    "def stem_tokens(tokens: list[str]) -> list[str]:\n",
    "    stemmer = PorterStemmer()\n",
    "    return list(map(lambda token: stemmer.stem(token), tokens))\n",
    "\n",
    "def process_text_to_tokens(text: str) -> list[str]:\n",
    "    tokens = remove_all_punctuation_lowercase(text=text)\n",
    "    tokens = tokenize(tokens)\n",
    "    tokens = remove_stop_words(tokens)\n",
    "    tokens = stem_tokens(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword search implementation\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, documents: dict = sample_dataset, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.documents = documents\n",
    "        self.index = None\n",
    "\n",
    "        # list of lists, each list one tokenized creator \n",
    "        self.tokenized_corpus = []\n",
    "\n",
    "        # save creator niches in same order in which added to corpus\n",
    "        self.ordered_creators = []\n",
    "\n",
    "        self.model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    def create_bm25_index(self):\n",
    "        # tokenized_creators = []\n",
    "        # creators_ordered = []\n",
    "        for creator, bio in self.documents.items():\n",
    "            tokenized_creator = process_text_to_tokens(bio)\n",
    "            self.tokenized_corpus.append(tokenized_creator)\n",
    "            self.ordered_creators.append(creator)\n",
    "\n",
    "        # self.tokenized_corpus = tokenized_creators\n",
    "        # self.ordered_creators = creators_ordered\n",
    "\n",
    "        self.index = BM25Okapi(self.tokenized_corpus)\n",
    "    \n",
    "    def bm25_search(self, query: str):\n",
    "        if self.index is None:\n",
    "            self.create_bm25_index()\n",
    "\n",
    "        tokenized_query = process_text_to_tokens(query)\n",
    "\n",
    "        # scores of all the Documents\n",
    "        scores = self.index.get_scores(tokenized_query)\n",
    "\n",
    "        results = []\n",
    "        for i, score in enumerate(scores):\n",
    "            creator = self.ordered_creators[i]\n",
    "            results.append((creator, score))\n",
    "\n",
    "        sorted_results = sorted(results, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "        # sort by score\n",
    "        return sorted_results[:4]\n",
    "    \n",
    "    def build_vector_db(self):\n",
    "        all_docs: list[Document] = []\n",
    "\n",
    "        # one Document per creator bio\n",
    "        for creator, bio in self.documents.items():\n",
    "            doc = Document(\n",
    "                page_content=bio, \n",
    "                metadata={\"source\": creator}\n",
    "            )\n",
    "            all_docs.append(doc)\n",
    "\n",
    "        # split into chunks, keep constant chunking config for now\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=20,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        all_chunks = text_splitter.split_documents(all_docs)\n",
    "        \n",
    "        # create vector store, vs retriever\n",
    "        CHROMA_DIR_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"- Creating a Vector Store at '{CHROMA_DIR_PATH.name}'...\")\n",
    "        vs = Chroma.from_documents(documents=all_chunks,\n",
    "                                   embedding=self.model,\n",
    "                                   collection_name=\"influencer-pokedex-corpus\",\n",
    "                                   persist_directory=str(CHROMA_DIR_PATH))\n",
    "        print(\"- Vector Store created ✔️\")\n",
    "        return vs.as_retriever(search_type=\"similarity\",\n",
    "                                    search_kwargs={\"k\":10})\n",
    "    \n",
    "    def load_or_create_vector_db(self) -> VectorStoreRetriever:\n",
    "        if CHROMA_DIR_PATH.exists() and (CHROMA_DIR_PATH / \"chroma.sqlite3\").exists():\n",
    "            print(f\"- Loading Vector Store from dsk at '{CHROMA_DIR_PATH.name}'...\")\n",
    "            # load the db, retriever\n",
    "            vs = Chroma(collection_name=\"influencer-pokedex-corpus\",\n",
    "                        embedding_function=self.model,\n",
    "                        persist_directory=str(CHROMA_DIR_PATH))\n",
    "            print(\"- Vector Store loaded ✔️\")\n",
    "            return vs.as_retriever(search_type=\"similarity\",\n",
    "                                   search_kwargs={\"k\":10})\n",
    "        else:\n",
    "            return self.build_vector_db()\n",
    "        \n",
    "    def semantic_search(self, query: str):\n",
    "        vs_retriever = self.load_or_create_vector_db()\n",
    "\n",
    "        # chunks\n",
    "        retrieved_docs = vs_retriever.invoke(query)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Calculating RRF, only need the rank\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            creator = doc.metadata[\"source\"]\n",
    "            results.append(creator)\n",
    "\n",
    "        return results\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
