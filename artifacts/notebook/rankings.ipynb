{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94de2897",
   "metadata": {},
   "source": [
    "# Ranking prototype (BM25 + Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c2b8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahullpanditaa/workstuff/github/assignments/influencer-pokedex/artifacts/notebook/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from rank_bm25 import BM25Okapi\n",
    "from pathlib import Path\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "DATA_DIR_PATH = Path.cwd().resolve() / \"data\"\n",
    "STOPWORDS_FILE_PATH = DATA_DIR_PATH / \"stopwords.txt\"\n",
    "CHROMA_DIR_PATH = Path.cwd().resolve() / \"vector_store\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e95965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset of 10 creators\n",
    "# dict where key = creator niche, value = creator bio\n",
    "\n",
    "sample_dataset = {\n",
    "    \"fashion_minimalist_style\": \"\"\"I post daily outfit inspirations, seasonal lookbooks, and minimalist fashion tips focused on affordable pieces. \n",
    "My recent content includes capsule wardrobe guides, styling basics, and color-coordination breakdowns. \n",
    "Instagram posts feature outfit photos, thrifted finds, and short styling reels.\n",
    "\"\"\",\n",
    "\n",
    "\"learning_to_code_journey\": \"\"\"I document my journey learning to code from scratch, posting study routines, project logs, and beginner-friendly explanations. \n",
    "Recent videos show progress building Python scripts, simple web apps, and solving DSA problems as a newcomer to tech. \n",
    "Instagram posts include accountability updates, motivational captions, and resources for absolute beginners.\n",
    "\"\"\",\n",
    "\n",
    "\"backend_go_engineer\": \"\"\"I share backend engineering tutorials in Go, focusing on concurrency, networking, microservice design, and Docker-based workflows. \n",
    "My recent content includes deep dives into goroutines, REST API design, production logging, and containerizing real-world services. \n",
    "Instagram posts feature short Go tips, dev memes, and workflow optimizations for backend developers.\n",
    "\"\"\",\n",
    "\n",
    "\"fitness_home_workouts\": \"\"\"I create daily home workout routines targeting fat loss, mobility, and functional strength using bodyweight or minimal equipment. \n",
    "My latest videos include 10-minute HIIT sessions, beginner full-body circuits, and nutrition tips for sustainable weight loss. \n",
    "Instagram posts share motivational progress photos, short workout reels, and simple healthy meal ideas.\n",
    "\"\"\",\n",
    "\n",
    "\"beauty_skincare_reviewer\": \"\"\"I review skincare products and routines for acne-prone, oily, and sensitive skin types, focusing on ingredient science. \n",
    "Recent videos compare retinol serums, sunscreen textures, exfoliants, and Korean skincare routines. \n",
    "Instagram posts share product flatlays, morning/night routines, and short ingredient breakdowns.\n",
    "\"\"\",\n",
    "\n",
    "\"personal_finance_educator\": \"\"\"I simplify personal finance topics such as index investing, budgeting systems, emergency funds, and tax optimization. \n",
    "My recent content includes ETF comparisons, beginner investment strategies, and monthly market breakdowns. \n",
    "Instagram posts include budgeting templates, money habits, and short explainers on compounding and inflation.\n",
    "\"\"\",\n",
    "\n",
    "\"healthy_cooking_mealprep\": \"\"\"I share easy, healthy recipes and weekly meal prep ideas designed for busy students and working professionals. \n",
    "My latest videos feature quick high-protein dinners, 15-minute lunches, and budget-friendly vegetarian meal preps. \n",
    "Instagram posts include grocery hauls, step-by-step meal reels, and simple nutrition tips.\n",
    "\"\"\",\n",
    "\n",
    "\"yoga_mindfulness_coach\": \"\"\"I guide yoga flows, stretching routines, and mindfulness practices aimed at improving mobility, posture, and mental clarity. \n",
    "Recent videos include morning yoga sessions, hip-opening flows, and breathwork techniques for stress relief. \n",
    "Instagram posts share short mobility drills, inspirational quotes, and meditation reminders.\n",
    "\"\"\",\n",
    "\n",
    "\"cloud_devops_engineer\": \"\"\"I break down cloud engineering concepts including AWS, Kubernetes, Docker, and CI/CD automation. \n",
    "My latest tutorials explain Terraform modules, EKS deployments, load balancing, and building secure production pipelines. \n",
    "Instagram posts include cloud diagrams, quick DevOps tips, and workflow comparisons for SREs and platform engineers.\n",
    "\"\"\",\n",
    "\n",
    "\"frontend_react_engineer\": \"\"\"I teach React, TypeScript, and modern frontend engineering with a focus on clean UI patterns and reusable components. \n",
    "Recent videos cover state management, hooks, responsive layouts, and building production-ready interfaces with React and Tailwind. \n",
    "On Instagram, I post quick JavaScript tips, UI design breakdowns, and small project showcases.\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b455c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for pre-processing text for OkapiBM25\n",
    "\n",
    "def _remove_all_punctuation_lowercase(text: str) -> str:\n",
    "    tt = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(tt).lower()\n",
    "\n",
    "def _tokenize(text: str) -> list[str]:\n",
    "    return text.lower().split()\n",
    "\n",
    "def _remove_stop_words(tokens: list[str]) -> list[str]:\n",
    "    with open(STOPWORDS_FILE_PATH, \"r\") as f:\n",
    "        stop_words = f.readlines()\n",
    "    \n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        result.append(token)\n",
    "\n",
    "    return result\n",
    "\n",
    "def _stem_tokens(tokens: list[str]) -> list[str]:\n",
    "    stemmer = PorterStemmer()\n",
    "    return list(map(lambda token: stemmer.stem(token), tokens))\n",
    "\n",
    "def _process_text_to_tokens(text: str) -> list[str]:\n",
    "    tokens = _remove_all_punctuation_lowercase(text=text)\n",
    "    tokens = _tokenize(tokens)\n",
    "    tokens = _remove_stop_words(tokens)\n",
    "    tokens = _stem_tokens(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword search implementation\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, documents: dict = sample_dataset, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.documents = documents\n",
    "        self.index = None\n",
    "\n",
    "        # list of lists, each list one tokenized creator \n",
    "        self.tokenized_corpus = []\n",
    "\n",
    "        # save creator niches in same order in which added to corpus\n",
    "        self.ordered_creators = []\n",
    "\n",
    "        self.model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    def _create_bm25_index(self):\n",
    "        # tokenized_creators = []\n",
    "        # creators_ordered = []\n",
    "        for creator, bio in self.documents.items():\n",
    "            tokenized_creator = _process_text_to_tokens(bio)\n",
    "            self.tokenized_corpus.append(tokenized_creator)\n",
    "            self.ordered_creators.append(creator)\n",
    "\n",
    "        # self.tokenized_corpus = tokenized_creators\n",
    "        # self.ordered_creators = creators_ordered\n",
    "\n",
    "        self.index = BM25Okapi(self.tokenized_corpus)\n",
    "    \n",
    "    def _bm25_search(self, query: str):\n",
    "        if self.index is None:\n",
    "            self._create_bm25_index()\n",
    "\n",
    "        tokenized_query = _process_text_to_tokens(query)\n",
    "\n",
    "        # scores of all the Documents\n",
    "        scores = self.index.get_scores(tokenized_query)\n",
    "\n",
    "        results = []\n",
    "        for i, score in enumerate(scores):\n",
    "            creator = self.ordered_creators[i]\n",
    "            results.append((creator, score))\n",
    "\n",
    "        # sort by score\n",
    "        sorted_results = sorted(results, key=lambda t: t[1], reverse=True)\n",
    "        \n",
    "        final_results = []\n",
    "        for c, _ in sorted_results:\n",
    "            final_results.append(c)\n",
    "        \n",
    "        return final_results[:5]\n",
    "    \n",
    "    def _build_vector_db(self):\n",
    "        all_docs: list[Document] = []\n",
    "\n",
    "        # one Document per creator bio\n",
    "        for creator, bio in self.documents.items():\n",
    "            doc = Document(\n",
    "                page_content=bio, \n",
    "                metadata={\"source\": creator}\n",
    "            )\n",
    "            all_docs.append(doc)\n",
    "\n",
    "        # split into chunks, keep constant chunking config for now\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=20,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        all_chunks = text_splitter.split_documents(all_docs)\n",
    "        \n",
    "        # create vector store, vs retriever\n",
    "        CHROMA_DIR_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"- Creating a Vector Store at '{CHROMA_DIR_PATH.name}'...\")\n",
    "        vs = Chroma.from_documents(documents=all_chunks,\n",
    "                                   embedding=self.model,\n",
    "                                   collection_name=\"influencer-pokedex-corpus\",\n",
    "                                   persist_directory=str(CHROMA_DIR_PATH))\n",
    "        print(\"- Vector Store created ✔️\")\n",
    "        return vs.as_retriever(search_type=\"similarity\",\n",
    "                                    search_kwargs={\"k\":10})\n",
    "    \n",
    "    def _load_or_create_vector_db(self) -> VectorStoreRetriever:\n",
    "        if CHROMA_DIR_PATH.exists() and (CHROMA_DIR_PATH / \"chroma.sqlite3\").exists():\n",
    "            print(f\"- Loading Vector Store from dsk at '{CHROMA_DIR_PATH.name}'...\")\n",
    "            # load the db, retriever\n",
    "            vs = Chroma(collection_name=\"influencer-pokedex-corpus\",\n",
    "                        embedding_function=self.model,\n",
    "                        persist_directory=str(CHROMA_DIR_PATH))\n",
    "            print(\"- Vector Store loaded ✔️\")\n",
    "            return vs.as_retriever(search_type=\"similarity\",\n",
    "                                   search_kwargs={\"k\":10})\n",
    "        else:\n",
    "            return self._build_vector_db()\n",
    "        \n",
    "    def _semantic_search(self, query: str):\n",
    "        vs_retriever = self._load_or_create_vector_db()\n",
    "\n",
    "        # chunks (most relevant)\n",
    "        retrieved_docs = vs_retriever.invoke(query)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # for calculating RRF, only need the rank\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            creator = doc.metadata[\"source\"]\n",
    "            results.append(creator)\n",
    "\n",
    "        return results[:5]\n",
    "    \n",
    "    def _hybrid_search(self, query):\n",
    "        # cmbine ranking instead of raw scores\n",
    "        # removes need for normlizing scores\n",
    "        # formula -> rrf = 1 / (k + rank)\n",
    "        # where k - rrf smoothing constant (lower gives more weight to top ranked results)\n",
    "\n",
    "        # both simply return a list of creator strings ranked\n",
    "        bm25_results = self._bm25_search(query=query)\n",
    "        semantic_results = self._semantic_search(query=query)\n",
    "\n",
    "        print(f\"Searching for '{query}'...\")\n",
    "        results = []\n",
    "        for creator, bio in self.documents.items():\n",
    "            bm25_rank = bm25_results.index(creator) + 1 if creator in bm25_results else len(bm25_results) + 1\n",
    "            semantic_rank = semantic_results.index(creator) + 1 if creator in semantic_results else len(semantic_results) + 1\n",
    "\n",
    "            rrf = 0.0\n",
    "            rrf += _rrf_score(rank=bm25_rank)\n",
    "            rrf += _rrf_score(rank=semantic_rank)\n",
    "\n",
    "            results.append({\n",
    "                \"creator\": creator,\n",
    "                \"bio\": bio[:100],\n",
    "                \"bm25\": bm25_rank,\n",
    "                \"semantic\": semantic_rank,\n",
    "                \"rrf\": rrf\n",
    "            })\n",
    "        sorted_results = sorted(results, key=lambda d: d[\"rrf\"], reverse=True)\n",
    "        return sorted_results\n",
    "    \n",
    "    def search(self, query: str):\n",
    "        results = self._hybrid_search(query=query)\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. {result['creator'].upper()}\")\n",
    "            print(f\"RRF Score: {result['rrf']:.4f}\")\n",
    "            print(f\"BM25 Rank: {result['bm25']}, Semantic Rank: {result['semantic']}\")\n",
    "            print(f\"{result['bio']}\")\n",
    "\n",
    "\n",
    "\n",
    "def _rrf_score(rank: int, k: int=60):\n",
    "    return 1 / (k + rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a02d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Creating a Vector Store at 'vector_store'...\n",
      "- Vector Store created ✔️\n",
      "Searching for 'fitness routine for beginners'...\n",
      "1. FITNESS_HOME_WORKOUTS\n",
      "RRF Score: 0.03252247488101534\n",
      "BM25 Rank: 2, Semantic Rank: 1\n",
      "I create daily home workout routines targeting fat loss, mobility, and functional strength using bod\n",
      "2. LEARNING_TO_CODE_JOURNEY\n",
      "RRF Score: 0.03177805800756621\n",
      "BM25 Rank: 1, Semantic Rank: 5\n",
      "I document my journey learning to code from scratch, posting study routines, project logs, and begin\n",
      "3. YOGA_MINDFULNESS_COACH\n",
      "RRF Score: 0.03125\n",
      "BM25 Rank: 4, Semantic Rank: 4\n",
      "I guide yoga flows, stretching routines, and mindfulness practices aimed at improving mobility, post\n",
      "4. BEAUTY_SKINCARE_REVIEWER\n",
      "RRF Score: 0.031024531024531024\n",
      "BM25 Rank: 3, Semantic Rank: 6\n",
      "I review skincare products and routines for acne-prone, oily, and sensitive skin types, focusing on \n",
      "5. PERSONAL_FINANCE_EDUCATOR\n",
      "RRF Score: 0.030536130536130537\n",
      "BM25 Rank: 5, Semantic Rank: 6\n",
      "I simplify personal finance topics such as index investing, budgeting systems, emergency funds, and \n",
      "6. FASHION_MINIMALIST_STYLE\n",
      "RRF Score: 0.030303030303030304\n",
      "BM25 Rank: 6, Semantic Rank: 6\n",
      "I post daily outfit inspirations, seasonal lookbooks, and minimalist fashion tips focused on afforda\n",
      "7. BACKEND_GO_ENGINEER\n",
      "RRF Score: 0.030303030303030304\n",
      "BM25 Rank: 6, Semantic Rank: 6\n",
      "I share backend engineering tutorials in Go, focusing on concurrency, networking, microservice desig\n",
      "8. HEALTHY_COOKING_MEALPREP\n",
      "RRF Score: 0.030303030303030304\n",
      "BM25 Rank: 6, Semantic Rank: 6\n",
      "I share easy, healthy recipes and weekly meal prep ideas designed for busy students and working prof\n",
      "9. CLOUD_DEVOPS_ENGINEER\n",
      "RRF Score: 0.030303030303030304\n",
      "BM25 Rank: 6, Semantic Rank: 6\n",
      "I break down cloud engineering concepts including AWS, Kubernetes, Docker, and CI/CD automation. \n",
      "My\n",
      "10. FRONTEND_REACT_ENGINEER\n",
      "RRF Score: 0.030303030303030304\n",
      "BM25 Rank: 6, Semantic Rank: 6\n",
      "I teach React, TypeScript, and modern frontend engineering with a focus on clean UI patterns and reu\n"
     ]
    }
   ],
   "source": [
    "# HYBRID SEARCHING\n",
    "example_query = \"fitness routine for beginners\"\n",
    "\n",
    "searcher = HybridSearch()\n",
    "searcher.search(example_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd73ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
